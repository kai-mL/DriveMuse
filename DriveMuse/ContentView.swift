// SwiftUI + MapKit + Speech Recognition + Gemini 2.0 Flash Áµ±ÂêàÁâà
import SwiftUI
import MapKit
import Speech
import AVFoundation

// MARK: - GeminiService: Google Gemini Flash API Âëº„Å≥Âá∫„Åó
class GeminiService: ObservableObject {
    static let shared = GeminiService()
    @Published var responseText: String = ""
    private let apiKey = "AIzaSyAmYOHti7xwpIch95mL1aEyb0gQcUNPTjc"
    private let model = "gemini-2.0-flash"

    func sendToGemini(_ text: String) async {
        // Á©∫ÊñáÂ≠ó„ÉÅ„Çß„ÉÉ„ÇØ
        let trimmed = text.trimmingCharacters(in: .whitespacesAndNewlines)
        guard !trimmed.isEmpty else { return }
        
        print("‚û°Ô∏è Sending to Gemini with text:", trimmed)

        // URL „Å®Ë™çË®º„Éò„ÉÉ„ÉÄ„Éº
        let url = URL(string: "https://generativelanguage.googleapis.com/v1beta/models/\(model):generateContent?key=\(apiKey)")!
        var request = URLRequest(url: url)
        request.httpMethod = "POST"
        request.setValue("application/json", forHTTPHeaderField: "Content-Type")

        // „É™„ÇØ„Ç®„Çπ„Éà„Éú„Éá„Ç£: Quickstart ‰ªïÊßò„Å´Ê∫ñÊã†
        let body: [String: Any] = [
            "contents": [
                [
                    "parts": [
                        ["text": trimmed]
                    ]
                ]
            ]
        ]
        request.httpBody = try? JSONSerialization.data(withJSONObject: body, options: [])

        do {
            let (data, _) = try await URLSession.shared.data(for: request)
            let raw = String(data: data, encoding: .utf8) ?? ""
            print("üåê Gemini raw response: \(raw)")

            if let json = try JSONSerialization.jsonObject(with: data) as? [String: Any],
               let candidates = json["candidates"] as? [[String: Any]],
               let first = candidates.first,
               let contentObj = first["content"] as? [String: Any],
               let parts = contentObj["parts"] as? [[String: Any]],
               let part0 = parts.first,
               let text = part0["text"] as? String {
                await MainActor.run {
                    self.responseText = text
                    print("‚úÖ Parsed Gemini reply:", text)
                }
            } else {
                print("‚ùóÔ∏è Failed to parse Gemini response structure.")
            }
        } catch {
            print("‚ùóÔ∏è Gemini API error:", error)
        }

    }
}

// MARK: - SpeechRecognizer: Èü≥Â£∞ÂÖ•Âäõ„Å®Ë™çË≠ò„ÇíÁÆ°ÁêÜ„Åô„Çã„ÇØ„É©„Çπ
class SpeechRecognizer: ObservableObject {
    @Published var recognizedText = ""
    @Published var isRecording = false
    @Published var transcriptLog: [String] = []
    @Published var authorizationStatus: SFSpeechRecognizerAuthorizationStatus = .notDetermined

    private let audioEngine = AVAudioEngine()
    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "ja-JP"))
    private var request: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?

    init() {
        AVAudioSession.sharedInstance().requestRecordPermission { _ in }
        SFSpeechRecognizer.requestAuthorization { status in
            DispatchQueue.main.async { self.authorizationStatus = status }
        }
    }

    func startRecording() {
        guard authorizationStatus == .authorized,
              let recognizer = speechRecognizer, recognizer.isAvailable,
              !audioEngine.isRunning else { return }
        recognizedText = ""

        let session = AVAudioSession.sharedInstance()
        try? session.setCategory(.record, mode: .measurement, options: .duckOthers)
        try? session.setActive(true, options: .notifyOthersOnDeactivation)

        request = SFSpeechAudioBufferRecognitionRequest()
        request?.shouldReportPartialResults = true

        recognitionTask = recognizer.recognitionTask(with: request!) { result, error in
            if let result = result {
                let text = result.bestTranscription.formattedString
                DispatchQueue.main.async {
                    self.recognizedText = text
                    if result.isFinal {
                        // Á©∫ÊñáÂ≠ó„Åß„Å™„Åë„Çå„Å∞ÈÄÅ‰ø°
                        let trimmed = text.trimmingCharacters(in: .whitespacesAndNewlines)
                        if !trimmed.isEmpty {
                            self.transcriptLog.append(trimmed)
                            print("üéô Final recognized text:", trimmed)
                            print("‚û°Ô∏è About to send to Gemini")
                            Task { await GeminiService.shared.sendToGemini(trimmed) }
                        }
                    }
                }
            }
            if error != nil || result?.isFinal == true {
                self.stopRecording()
            }
        }

        let input = audioEngine.inputNode
        let format = input.outputFormat(forBus: 0)
        input.installTap(onBus: 0, bufferSize: 1024, format: format) { buffer, _ in
            self.request?.append(buffer)
        }

        audioEngine.prepare()
        try? audioEngine.start()
        DispatchQueue.main.async { self.isRecording = true }
    }

    func stopRecording() {
        audioEngine.stop()
        audioEngine.inputNode.removeTap(onBus: 0)
        request?.endAudio()
        // ‚û°Ô∏è „Åì„Åì„Åß„ÅØ cancel „Åó„Å™„ÅÑ
        // recognitionTask?.cancel()
        DispatchQueue.main.async { self.isRecording = false }
        try? AVAudioSession.sharedInstance().setActive(false)
    }

    // ÂøÖË¶Å„Å™„Çâ„ÄÅÊúÄÁµÇÁöÑ„Å´„Çø„Çπ„ÇØ„ÇíÁ†¥Ê£Ñ„Åô„Çã„É°„ÇΩ„ÉÉ„Éâ„ÇíÂà•„Å´Áî®ÊÑè
    func cleanUpRecognition() {
        recognitionTask?.cancel()
        recognitionTask = nil
    }

}
// MARK: - TouristMapView: MapKitË°®Á§∫ÈÉ®ÂàÜÔºàÂ§âÊõ¥„Å™„ÅóÔºâ
struct TouristMapView: UIViewRepresentable {
    @Binding var region: MKCoordinateRegion
    func makeCoordinator() -> Coordinator { Coordinator(self) }
    func makeUIView(context: Context) -> MKMapView {
        let mapView = MKMapView()
        mapView.delegate = context.coordinator
        context.coordinator.mapView = mapView
        mapView.setRegion(region, animated: false)
        let config = MKStandardMapConfiguration(elevationStyle: .flat, emphasisStyle: .default)
        let categories: [MKPointOfInterestCategory] = [.amusementPark, .aquarium, .beach, .campground, .castle, .fairground, .fortress, .nationalMonument, .nationalPark, .planetarium, .spa, .zoo]
        config.pointOfInterestFilter = MKPointOfInterestFilter(including: categories)
        mapView.preferredConfiguration = config
        let station = MKPointAnnotation()
        station.coordinate = CLLocationCoordinate2D(latitude: 35.6585, longitude: 139.7013)
        station.title = "Ê∏ãË∞∑ÈßÖ"
        mapView.addAnnotation(station)
        context.coordinator.searchRestaurants()
        return mapView
    }
    func updateUIView(_ uiView: MKMapView, context: Context) {
        uiView.setRegion(region, animated: true)
    }
    class Coordinator: NSObject, MKMapViewDelegate {
        var parent: TouristMapView
        weak var mapView: MKMapView?
        init(_ parent: TouristMapView) { self.parent = parent }
        func mapView(_ mapView: MKMapView, regionDidChangeAnimated animated: Bool) {
            parent.region = mapView.region
            searchRestaurants()
        }
        func searchRestaurants() {
            guard let mapView = mapView else { return }
            let old = mapView.annotations.filter { $0.subtitle??.hasPrefix("Ë©ï‰æ°:") ?? false }
            mapView.removeAnnotations(old)
            let req = MKLocalSearch.Request()
            req.naturalLanguageQuery = "„É¨„Çπ„Éà„É©„É≥"
            req.region = mapView.region
            MKLocalSearch(request: req).start { resp, _ in
                guard let items = resp?.mapItems else { return }
                let results = items.sorted {
                    MKMapPoint($0.placemark.coordinate).distance(to: MKMapPoint(mapView.centerCoordinate)) <
                    MKMapPoint($1.placemark.coordinate).distance(to: MKMapPoint(mapView.centerCoordinate))
                }.prefix(5)
                for item in results {
                    let ann = MKPointAnnotation()
                    ann.coordinate = item.placemark.coordinate
                    ann.title = item.name
                    let dist = MKMapPoint(mapView.centerCoordinate).distance(to: MKMapPoint(item.placemark.coordinate))
                    let rating = max(1.0, 5.0 - dist / 1000.0)
                    ann.subtitle = String(format: "Ë©ï‰æ°: %.1f ‚≠êÔ∏è", rating)
                    mapView.addAnnotation(ann)
                }
            }
        }
        func mapView(_ mapView: MKMapView, viewFor annotation: MKAnnotation) -> MKAnnotationView? {
            if annotation is MKUserLocation { return nil }
            let id = annotation.title == "Ê∏ãË∞∑ÈßÖ" ? "station" : "restaurant"
            if id == "station" {
                var view = mapView.dequeueReusableAnnotationView(withIdentifier: id) as? MKPinAnnotationView
                if view == nil {
                    view = MKPinAnnotationView(annotation: annotation, reuseIdentifier: id)
                    view?.pinTintColor = .red; view?.canShowCallout = true
                } else { view?.annotation = annotation }
                return view
            } else {
                var view = mapView.dequeueReusableAnnotationView(withIdentifier: id) as? MKMarkerAnnotationView
                if view == nil {
                    view = MKMarkerAnnotationView(annotation: annotation, reuseIdentifier: id)
                    view?.glyphText = "üç¥"; view?.markerTintColor = .orange; view?.canShowCallout = true
                } else { view?.annotation = annotation }
                return view
            }
        }
    }
}

// MARK: - ContentView: Âú∞Âõ≥ + Èü≥Â£∞ + Gemini„É¨„Çπ„Éù„É≥„Çπ
struct ContentView: View {
    @State private var region = MKCoordinateRegion(
        center: CLLocationCoordinate2D(latitude: 35.6585, longitude: 139.7013),
        latitudinalMeters: 1000, longitudinalMeters: 1000
    )
    @StateObject private var speech = SpeechRecognizer()
    @StateObject private var gemini = GeminiService.shared
    
    // Ë™≠„Åø‰∏ä„ÅíÁî®„Ç∑„É≥„Çª„Çµ„Ç§„Ç∂„Éº„Çí‰øùÊåÅ
    @State private var synthesizer = AVSpeechSynthesizer()

    var body: some View {
            ZStack(alignment: .bottomTrailing) {
                TouristMapView(region: $region)
                    .ignoresSafeArea()

                VStack(alignment: .trailing, spacing: 12) {
                    if !gemini.responseText.isEmpty {
                        Text("Gemini: \(gemini.responseText)")
                            .padding(8)
                            .background(Color.white.opacity(0.8))
                            .cornerRadius(8)
                    }
                    if speech.isRecording {
                        Text(speech.recognizedText)
                            .padding(8)
                            .background(Color.white.opacity(0.8))
                            .cornerRadius(8)
                    }
                    Button(action: {
                        speech.isRecording ? speech.stopRecording() : speech.startRecording()
                    }) {
                        Label(speech.isRecording ? "Èå≤Èü≥ÂÅúÊ≠¢" : "Ë©±„Åô", systemImage: speech.isRecording ? "mic.fill" : "mic")
                            .font(.title2).padding()
                            .background(speech.isRecording ? Color.red : Color.blue)
                            .foregroundColor(.white).clipShape(Capsule())
                    }
                    .padding(.bottom, 20)
                }
                .padding(.trailing, 20)
            }
            // gemini.responseText „ÅåÂ§â„Çè„Å£„Åü„ÇâË™≠„Åø‰∏ä„Åí„Çã
            .onChange(of: gemini.responseText) { newText in
                // 1) AVAudioSession „ÇíÂÜçÁîü„É¢„Éº„Éâ„Å´
                let session = AVAudioSession.sharedInstance()
                try? session.setCategory(.playback, mode: .default)
                try? session.setActive(true)

                // 2) Ë™≠„Åø‰∏ä„ÅíÁî®„ÅÆ Utterance „Çí‰ΩúÊàê
                let utterance = AVSpeechUtterance(string: newText)
                utterance.voice = AVSpeechSynthesisVoice(language: "ja-JP")
                utterance.rate = AVSpeechUtteranceDefaultSpeechRate

                // 3) Ë™≠„Åø‰∏ä„ÅíÈñãÂßã
                synthesizer.speak(utterance)
            }

    }
}

