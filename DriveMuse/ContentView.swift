<<<<<<< HEAD

import SwiftUI
import MapKit

struct ContentView: View {
    
    @State private var region = MKCoordinateRegion(
            //Mapã®ä¸­å¿ƒã®ç·¯åº¦çµŒåº¦
            center: CLLocationCoordinate2D(latitude: 35.6585,
                                           longitude: 139.7013),
            //ç·¯åº¦ã®è¡¨ç¤ºé ˜åŸŸ(m)
            latitudinalMeters: 750,
            //çµŒåº¦ã®è¡¨ç¤ºé ˜åŸŸ(m)
            longitudinalMeters: 750
    )
    var body: some View {
        Map(coordinateRegion: $region,
            //Mapã®æ“ä½œã®æŒ‡å®š
            interactionModes: .pan,
            //ç¾åœ¨åœ°ã®è¡¨ç¤º
            showsUserLocation: true,
            //ç¾åœ¨åœ°ã®è¿½å¾“
            userTrackingMode: .constant(MapUserTrackingMode.follow)
        )
        .task(){
            //ä½ç½®æƒ…å ±ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’è¦æ±‚
            let manager = CLLocationManager()
            manager.requestWhenInUseAuthorization()
        }
    }
}
=======
// SwiftUI + MapKit + Speech Recognition + Gemini 2.0 Flash çµ±åˆç‰ˆ
import SwiftUI
import MapKit
import Speech
import AVFoundation

// MARK: - GeminiService: Google Gemini Flash API å‘¼ã³å‡ºã—
class GeminiService: ObservableObject {
    static let shared = GeminiService()
    @Published var responseText: String = ""
    private let apiKey = "AIzaSyAmYOHti7xwpIch95mL1aEyb0gQcUNPTjc"
    private let model = "gemini-2.0-flash"

    func sendToGemini(_ text: String) async {
        // ç©ºæ–‡å­—ãƒã‚§ãƒƒã‚¯
        let trimmed = text.trimmingCharacters(in: .whitespacesAndNewlines)
        guard !trimmed.isEmpty else { return }
        
        print("âž¡ï¸ Sending to Gemini with text:", trimmed)

        // URL ã¨èªè¨¼ãƒ˜ãƒƒãƒ€ãƒ¼
        let url = URL(string: "https://generativelanguage.googleapis.com/v1beta/models/\(model):generateContent?key=\(apiKey)")!
        var request = URLRequest(url: url)
        request.httpMethod = "POST"
        request.setValue("application/json", forHTTPHeaderField: "Content-Type")

        // ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£: Quickstart ä»•æ§˜ã«æº–æ‹ 
        let body: [String: Any] = [
            "contents": [
                [
                    "parts": [
                        ["text": trimmed]
                    ]
                ]
            ]
        ]
        request.httpBody = try? JSONSerialization.data(withJSONObject: body, options: [])

        do {
            let (data, _) = try await URLSession.shared.data(for: request)
            let raw = String(data: data, encoding: .utf8) ?? ""
            print("ðŸŒ Gemini raw response: \(raw)")

            if let json = try JSONSerialization.jsonObject(with: data) as? [String: Any],
               let candidates = json["candidates"] as? [[String: Any]],
               let first = candidates.first,
               let contentObj = first["content"] as? [String: Any],
               let parts = contentObj["parts"] as? [[String: Any]],
               let part0 = parts.first,
               let text = part0["text"] as? String {
                await MainActor.run {
                    self.responseText = text
                    print("âœ… Parsed Gemini reply:", text)
                }
            } else {
                print("â—ï¸ Failed to parse Gemini response structure.")
            }
        } catch {
            print("â—ï¸ Gemini API error:", error)
        }

    }
}

// MARK: - SpeechRecognizer: éŸ³å£°å…¥åŠ›ã¨èªè­˜ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹
class SpeechRecognizer: ObservableObject {
    @Published var recognizedText = ""
    @Published var isRecording = false
    @Published var transcriptLog: [String] = []
    @Published var authorizationStatus: SFSpeechRecognizerAuthorizationStatus = .notDetermined

    private let audioEngine = AVAudioEngine()
    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "ja-JP"))
    private var request: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?

    init() {
        AVAudioSession.sharedInstance().requestRecordPermission { _ in }
        SFSpeechRecognizer.requestAuthorization { status in
            DispatchQueue.main.async { self.authorizationStatus = status }
        }
    }

    func startRecording() {
        guard authorizationStatus == .authorized,
              let recognizer = speechRecognizer, recognizer.isAvailable,
              !audioEngine.isRunning else { return }
        recognizedText = ""

        let session = AVAudioSession.sharedInstance()
        try? session.setCategory(.record, mode: .measurement, options: .duckOthers)
        try? session.setActive(true, options: .notifyOthersOnDeactivation)

        request = SFSpeechAudioBufferRecognitionRequest()
        request?.shouldReportPartialResults = true

        recognitionTask = recognizer.recognitionTask(with: request!) { result, error in
            if let result = result {
                let text = result.bestTranscription.formattedString
                DispatchQueue.main.async {
                    self.recognizedText = text
                    if result.isFinal {
                        // ç©ºæ–‡å­—ã§ãªã‘ã‚Œã°é€ä¿¡
                        let trimmed = text.trimmingCharacters(in: .whitespacesAndNewlines)
                        if !trimmed.isEmpty {
                            self.transcriptLog.append(trimmed)
                            print("ðŸŽ™ Final recognized text:", trimmed)
                            print("âž¡ï¸ About to send to Gemini")
                            Task { await GeminiService.shared.sendToGemini(trimmed) }
                        }
                    }
                }
            }
            if error != nil || result?.isFinal == true {
                self.stopRecording()
            }
        }

        let input = audioEngine.inputNode
        let format = input.outputFormat(forBus: 0)
        input.installTap(onBus: 0, bufferSize: 1024, format: format) { buffer, _ in
            self.request?.append(buffer)
        }

        audioEngine.prepare()
        try? audioEngine.start()
        DispatchQueue.main.async { self.isRecording = true }
    }

    func stopRecording() {
        audioEngine.stop()
        audioEngine.inputNode.removeTap(onBus: 0)
        request?.endAudio()
        // âž¡ï¸ ã“ã“ã§ã¯ cancel ã—ãªã„
        // recognitionTask?.cancel()
        DispatchQueue.main.async { self.isRecording = false }
        try? AVAudioSession.sharedInstance().setActive(false)
    }

    // å¿…è¦ãªã‚‰ã€æœ€çµ‚çš„ã«ã‚¿ã‚¹ã‚¯ã‚’ç ´æ£„ã™ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰ã‚’åˆ¥ã«ç”¨æ„
    func cleanUpRecognition() {
        recognitionTask?.cancel()
        recognitionTask = nil
    }

}
// MARK: - TouristMapView: MapKitè¡¨ç¤ºéƒ¨åˆ†ï¼ˆå¤‰æ›´ãªã—ï¼‰
struct TouristMapView: UIViewRepresentable {
    @Binding var region: MKCoordinateRegion
    func makeCoordinator() -> Coordinator { Coordinator(self) }
    func makeUIView(context: Context) -> MKMapView {
        let mapView = MKMapView()
        mapView.delegate = context.coordinator
        context.coordinator.mapView = mapView
        mapView.setRegion(region, animated: false)
        let config = MKStandardMapConfiguration(elevationStyle: .flat, emphasisStyle: .default)
        let categories: [MKPointOfInterestCategory] = [.amusementPark, .aquarium, .beach, .campground, .castle, .fairground, .fortress, .nationalMonument, .nationalPark, .planetarium, .spa, .zoo]
        config.pointOfInterestFilter = MKPointOfInterestFilter(including: categories)
        mapView.preferredConfiguration = config
        let station = MKPointAnnotation()
        station.coordinate = CLLocationCoordinate2D(latitude: 35.6585, longitude: 139.7013)
        station.title = "æ¸‹è°·é§…"
        mapView.addAnnotation(station)
        context.coordinator.searchRestaurants()
        return mapView
    }
    func updateUIView(_ uiView: MKMapView, context: Context) {
        uiView.setRegion(region, animated: true)
    }
    class Coordinator: NSObject, MKMapViewDelegate {
        var parent: TouristMapView
        weak var mapView: MKMapView?
        init(_ parent: TouristMapView) { self.parent = parent }
        func mapView(_ mapView: MKMapView, regionDidChangeAnimated animated: Bool) {
            parent.region = mapView.region
            searchRestaurants()
        }
        func searchRestaurants() {
            guard let mapView = mapView else { return }
            let old = mapView.annotations.filter { $0.subtitle??.hasPrefix("è©•ä¾¡:") ?? false }
            mapView.removeAnnotations(old)
            let req = MKLocalSearch.Request()
            req.naturalLanguageQuery = "ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³"
            req.region = mapView.region
            MKLocalSearch(request: req).start { resp, _ in
                guard let items = resp?.mapItems else { return }
                let results = items.sorted {
                    MKMapPoint($0.placemark.coordinate).distance(to: MKMapPoint(mapView.centerCoordinate)) <
                    MKMapPoint($1.placemark.coordinate).distance(to: MKMapPoint(mapView.centerCoordinate))
                }.prefix(5)
                for item in results {
                    let ann = MKPointAnnotation()
                    ann.coordinate = item.placemark.coordinate
                    ann.title = item.name
                    let dist = MKMapPoint(mapView.centerCoordinate).distance(to: MKMapPoint(item.placemark.coordinate))
                    let rating = max(1.0, 5.0 - dist / 1000.0)
                    ann.subtitle = String(format: "è©•ä¾¡: %.1f â­ï¸", rating)
                    mapView.addAnnotation(ann)
                }
            }
        }
        func mapView(_ mapView: MKMapView, viewFor annotation: MKAnnotation) -> MKAnnotationView? {
            if annotation is MKUserLocation { return nil }
            let id = annotation.title == "æ¸‹è°·é§…" ? "station" : "restaurant"
            if id == "station" {
                var view = mapView.dequeueReusableAnnotationView(withIdentifier: id) as? MKPinAnnotationView
                if view == nil {
                    view = MKPinAnnotationView(annotation: annotation, reuseIdentifier: id)
                    view?.pinTintColor = .red; view?.canShowCallout = true
                } else { view?.annotation = annotation }
                return view
            } else {
                var view = mapView.dequeueReusableAnnotationView(withIdentifier: id) as? MKMarkerAnnotationView
                if view == nil {
                    view = MKMarkerAnnotationView(annotation: annotation, reuseIdentifier: id)
                    view?.glyphText = "ðŸ´"; view?.markerTintColor = .orange; view?.canShowCallout = true
                } else { view?.annotation = annotation }
                return view
            }
        }
    }
}

// MARK: - ContentView: åœ°å›³ + éŸ³å£° + Geminiãƒ¬ã‚¹ãƒãƒ³ã‚¹
struct ContentView: View {
    @State private var region = MKCoordinateRegion(
        center: CLLocationCoordinate2D(latitude: 35.6585, longitude: 139.7013),
        latitudinalMeters: 1000, longitudinalMeters: 1000
    )
    @StateObject private var speech = SpeechRecognizer()
    @StateObject private var gemini = GeminiService.shared
    
    // èª­ã¿ä¸Šã’ç”¨ã‚·ãƒ³ã‚»ã‚µã‚¤ã‚¶ãƒ¼ã‚’ä¿æŒ
    @State private var synthesizer = AVSpeechSynthesizer()

    var body: some View {
            ZStack(alignment: .bottomTrailing) {
                TouristMapView(region: $region)
                    .ignoresSafeArea()

                VStack(alignment: .trailing, spacing: 12) {
                    if !gemini.responseText.isEmpty {
                        Text("Gemini: \(gemini.responseText)")
                            .padding(8)
                            .background(Color.white.opacity(0.8))
                            .cornerRadius(8)
                    }
                    if speech.isRecording {
                        Text(speech.recognizedText)
                            .padding(8)
                            .background(Color.white.opacity(0.8))
                            .cornerRadius(8)
                    }
                    Button(action: {
                        speech.isRecording ? speech.stopRecording() : speech.startRecording()
                    }) {
                        Label(speech.isRecording ? "éŒ²éŸ³åœæ­¢" : "è©±ã™", systemImage: speech.isRecording ? "mic.fill" : "mic")
                            .font(.title2).padding()
                            .background(speech.isRecording ? Color.red : Color.blue)
                            .foregroundColor(.white).clipShape(Capsule())
                    }
                    .padding(.bottom, 20)
                }
                .padding(.trailing, 20)
            }
            // gemini.responseText ãŒå¤‰ã‚ã£ãŸã‚‰èª­ã¿ä¸Šã’ã‚‹
            .onChange(of: gemini.responseText) { newText in
                // 1) AVAudioSession ã‚’å†ç”Ÿãƒ¢ãƒ¼ãƒ‰ã«
                let session = AVAudioSession.sharedInstance()
                try? session.setCategory(.playback, mode: .default)
                try? session.setActive(true)

                // 2) èª­ã¿ä¸Šã’ç”¨ã® Utterance ã‚’ä½œæˆ
                let utterance = AVSpeechUtterance(string: newText)
                utterance.voice = AVSpeechSynthesisVoice(language: "ja-JP")
                utterance.rate = AVSpeechUtteranceDefaultSpeechRate

                // 3) èª­ã¿ä¸Šã’é–‹å§‹
                synthesizer.speak(utterance)
            }

    }
}

>>>>>>> ponyo
